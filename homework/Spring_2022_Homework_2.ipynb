{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spring 2022 Homework 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlv8esDGMj0h"
      },
      "source": [
        "# CIS 545 Homework 2: SQL\n",
        "## Due: Monday, February 21, 2022 by 10pm \n",
        "### Worth 100 points in total\n",
        "\n",
        "Welcome to Homework 2! By now, you should be familiar with the world of data science and the Pandas library. This assignment will focus on broadening both of these horizons by covering hierarchical data, graphs, and traversing relationships as well as a new tool to add to your data science arsenal: SQL.\n",
        "\n",
        "Through this homework, we will familiarize ourselves with SQL (specifically **pandasql**) and explore a dataset that involves movie statistics and reviews courtesy of Rotten Tomatoes. We will finish off the homework with some text analysis.\n",
        "\n",
        "We are introducing a lot of new things in this homework, and it is often where students start to get lost in the data science sauce, so we **strongly** encourage you to review the slides/material as you work through this assignment and will try to link the most relevant sections!\n",
        "\n",
        "**Before you Begin**\n",
        "- Be sure to click \"Copy to Drive\" to make sure you are working on your own personal version of the homework\n",
        "- Read the Piazza and FAQ for updates! If you have been stuck, chances are other students are too! We don't want you to waste away for two hours trying to get that last point on the autograder so do check Piazza for similar struggles or even homework bugs that will be clarified in the FAQ :) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0hcZWDcqCUL"
      },
      "source": [
        "## Part 0: Libraries and Set Up Jargon (The usual wall of imports)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylkUtozb2Oc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c890de8-84fb-4080-d93d-6826b1894b7c"
      },
      "source": [
        "!pip3 install penngrader\n",
        "!pip install pandasql"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: penngrader in /usr/local/lib/python3.7/dist-packages (0.4)\n",
            "Requirement already satisfied: pandasql in /usr/local/lib/python3.7/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.21.5)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.4.31)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.15.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (4.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (3.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.1.5"
      ],
      "metadata": {
        "id": "goglom5eSRPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "8346c9af-ce03-44da-974d-34c61f9f3189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAXVD0Lo454x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02064456-2d5b-439e-8053-d3b1d0c96275"
      },
      "source": [
        "from penngrader.grader import *\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import re\n",
        "import pandasql as ps #SQL on Pandas Dataframe\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt \n",
        "from collections import Counter\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyTldwTj4aY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42de8e70-a29d-45bf-de3f-9fadd816438f"
      },
      "source": [
        "# Three datasets we're using\n",
        "! wget -nc https://storage.googleapis.com/penn-cis545/rotten_tomatoes_movies.csv\n",
        "! wget -nc https://storage.googleapis.com/penn-cis545/rotten_tomatoes_critic_reviews.csv\n",
        "! wget -nc https://storage.googleapis.com/penn-cis545/MoviesOnStreamingPlatforms.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-21 16:09:46--  https://storage.googleapis.com/penn-cis545/rotten_tomatoes_movies.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.123.128, 142.250.98.128, 142.250.97.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.123.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17229548 (16M) [text/csv]\n",
            "Saving to: ‘rotten_tomatoes_movies.csv’\n",
            "\n",
            "\r          rotten_to   0%[                    ]       0  --.-KB/s               \rrotten_tomatoes_mov 100%[===================>]  16.43M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-21 16:09:46 (155 MB/s) - ‘rotten_tomatoes_movies.csv’ saved [17229548/17229548]\n",
            "\n",
            "--2022-02-21 16:09:46--  https://storage.googleapis.com/penn-cis545/rotten_tomatoes_critic_reviews.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.216.128, 173.194.217.128, 173.194.218.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.216.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 226049903 (216M) [text/csv]\n",
            "Saving to: ‘rotten_tomatoes_critic_reviews.csv’\n",
            "\n",
            "rotten_tomatoes_cri 100%[===================>] 215.58M   195MB/s    in 1.1s    \n",
            "\n",
            "2022-02-21 16:09:47 (195 MB/s) - ‘rotten_tomatoes_critic_reviews.csv’ saved [226049903/226049903]\n",
            "\n",
            "--2022-02-21 16:09:47--  https://storage.googleapis.com/penn-cis545/MoviesOnStreamingPlatforms.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.11.128, 74.125.31.128, 74.125.141.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.11.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 506071 (494K) [text/csv]\n",
            "Saving to: ‘MoviesOnStreamingPlatforms.csv’\n",
            "\n",
            "MoviesOnStreamingPl 100%[===================>] 494.21K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-02-21 16:09:47 (125 MB/s) - ‘MoviesOnStreamingPlatforms.csv’ saved [506071/506071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5NogkmdhhQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8bde38-912c-4320-fcd1-1947f6a0f766"
      },
      "source": [
        "print(pd.__version__)\n",
        "# Make sure it's 1.1.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyWoMn6pxSC"
      },
      "source": [
        "### PennGrader Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ds2HHSkpvBO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "1fcfb1d5-2aba-4272-f9c0-5a052c5d9a78"
      },
      "source": [
        "# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW \n",
        "# TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = # YOUR PENN-ID GOES HERE AS AN INTEGER #"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-1c01d2f70249>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    STUDENT_ID = # YOUR PENN-ID GOES HERE AS AN INTEGER #\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAsLYJGotKbO"
      },
      "source": [
        "grader = PennGrader(homework_id = 'CIS545_Spring_2022_HW2', student_id = STUDENT_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzZ6JNnKLplQ"
      },
      "source": [
        "\n",
        "\n",
        "# Section 1: Welcome to the movies!\n",
        "\n",
        "<br>\n",
        "<center><img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Rotten_Tomatoes_logo.svg/2880px-Rotten_Tomatoes_logo.svg.png\" width= \"500\" align =\"center\"/></center>\n",
        "<br>\n",
        "\n",
        "I'm sure everyone has looked on the internet at some point in their lives for reviews, whether it be for products, locations, or services. Reviews are important because they give us knowledge about how good (or bad) the reviewed item was, at least in the past. Rotten Tomatoes contains one of the most comprehensive databases of movie/TV show reviews. Its name comes from how audience members in the past tended to throw rotten tomatoes when they disliked a stage performance. Formed in 1998 by a bunch of students, it has withstood the test of time, and the rest is history... that you can read about yourself on [Wikipedia](https://en.wikipedia.org/wiki/Rotten_Tomatoes). \n",
        "\n",
        "\n",
        "In this homework, we'll be exploring some data about movies including:\n",
        "\n",
        "*   Movies: data about movies in the Rotten Tomatoes database.\n",
        "\n",
        "*   Reviews: data about each review that was posted on Rotten Tomatoes.\n",
        "\n",
        "*   Streamed movies: data about the movies that are currently available on major streaming platforms. \n",
        "\n",
        "\n",
        "We'll be parsing this data into dataframes and relations, and then exploring how to query and assemble the tables into results. We will primarily be using PandaSQL, but for some of the initial questions, we will ask you to perform the same operations in Pandas as well, so as to familiarize you with the differences and similarities of the two. `"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oENmSeFkFRCo"
      },
      "source": [
        "## Part 1: Load & Process our Datasets [9 points total]\n",
        "\n",
        "Before we get into the data, we first need to load and clean our datasets. \n",
        "\n",
        "**TODO**:\n",
        "* Load and save the `rotten_tomatoes_movies.csv` to a dataframe called `movies_df`.\n",
        "* Load and save the `rotten_tomatoes_critic_reviews.csv` to a dataframe called `reviews_df`.\n",
        "* Load and save the `MoviesOnStreamingPlatforms.csv` to a dataframe called `streaming_df` without the index column being included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMn38A25Vt8"
      },
      "source": [
        "# TODO: Import the datasets to pandas dataframes -- make sure the dataframes are named correctly! \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LLY5EEmNxnr"
      },
      "source": [
        "# view movies_df to make sure the import was successful\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py_XvfZiNzXx"
      },
      "source": [
        "# view reviews_df to make sure the import was successful\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view streaming_df to make sure the import was successful\n"
      ],
      "metadata": {
        "id": "hZJwhz1lx4ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPitr7eRJfO0"
      },
      "source": [
        "### 1.1 Data Preprocessing\n",
        "\n",
        "Next, we are going to want to clean up our dataframes, namely `movies_df` and `reviews_df`, by 1) fixing column names, 2) changing datatypes, 3) cleaning text, and 4) handling nulls.\n",
        "\n",
        "First, let us view the first few rows of `movies_df`. You may also call `.info()` to view the specifics of the dataframe. This is a good first step to take for Exploratory Data Analysis (EDA)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# view info information regarding movies_df\n",
        "movies_df.info()"
      ],
      "metadata": {
        "id": "2nCrp9Z2SCRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ukI21sPkYV"
      },
      "source": [
        "#### 1.1.1 Cleaning `movies_df`\n",
        "\n",
        "`.info()` gives us meaningful information regarding columns, their types, and the amount of nulls, based on which we can now clean our dataframe. \n",
        "\n",
        "**TODO**:\n",
        "* Drop the columns `critics_consensus` and `production_company`.\n",
        "* Replace all nulls in the column `genres` with the string `\"No Genre\"`.\n",
        "* Drop all rows that have at least one null.\n",
        "* Cast columns `original_release_date`,`streaming_release_date` to be type `datetime64[ns]`.\n",
        "* Cast columns `runtime`,`tomatometer_count`, `tomatometer_rating`, `audience_count`, and `audience_rating` to be type `int64`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMmQTXV7P4YK"
      },
      "source": [
        "# TODO: clean movies_df\n",
        "movies_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SYxGtgOQKLY"
      },
      "source": [
        "# 3 points\n",
        "grader.grade(test_case_id = 'test_cleaning_movies', answer = movies_df.head(1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.2 Processing Genres\n",
        "\n",
        "`movies_df` also contains a `genres` column that represents an exhaustive list of all the genres a particular movie falls under. The comma-separated string format this column is in isn't too useful to us at the moment...\n",
        "\n",
        "**TODO**:\n",
        "- Copy over `movies_df` into a new dataframe called `exploded_movies_df`. \n",
        "- Split the genres listed such that each row contains only one listed genre (if a particular movie has 2 genres, that row will appear twice for each of the genres) - call this column `genre`\n",
        "- Strip the `genre` column of any leading or trailing whitespaces\n",
        "\n",
        "\n",
        "**Hint**: See the `.explode()` and `.strip()` functions"
      ],
      "metadata": {
        "id": "aI_zsI3hNJCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create new dataframe and genre column\n",
        "exploded_movies_df = ..."
      ],
      "metadata": {
        "id": "ZOfBFzD7NK8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 points\n",
        "grader.grade(test_case_id = 'test_genre_processing', answer = exploded_movies_df.head(1000))"
      ],
      "metadata": {
        "id": "sKA2_ScM6nWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38JSvyEGCGYN"
      },
      "source": [
        "#### 1.1.3 Cleaning `reviews_df`\n",
        "\n",
        "Then, let's take a look at cleaning `reviews_df`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#view info of reviews_df\n"
      ],
      "metadata": {
        "id": "vKv7Gdi_CGYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz3Xx84WCGYU"
      },
      "source": [
        "\n",
        "**TODO**:\n",
        "* Drop all rows that have a null value in the column `review_score` or `review_content`.\n",
        "* Replace each null in the `critic_name` column with the string `\"Anonymous\"`.\n",
        "* Convert column `review_date` into type `datetime64[ns]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfBTqJs3CGYV"
      },
      "source": [
        "#TODO: Clean reviews_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PacFnS9JCGYW"
      },
      "source": [
        "# 3 points\n",
        "grader.grade(test_case_id = 'test_cleaning_reviews', answer = reviews_df.head(1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYp9fW_SvG3g"
      },
      "source": [
        "### 1.2 Your Sandbox \n",
        "\n",
        ".info() is just one of many basic tools that you can use for Exploratory Data Analysis (EDA). Instead of throwing you straight into the deep end, we wanted to give you a chance to take some time and explore the data on your own. **This section is not graded**, so for the speedrunners out there feel free to just jump in, but we wanted to at least give you a small space to utilize your EDA toolkit to familiarize yourself with all the data you just downloaded.\n",
        "\n",
        "Some suggestions to get you started:\n",
        "- `df.head()`\n",
        "- `df.describe()`\n",
        "- `Series.unique()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U429NTI5RY4a"
      },
      "source": [
        "# Your EDA here! Feel free to add more cells\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHFdRtQKLbti"
      },
      "source": [
        "## Part 2: Exploring the Data with PandasSQL (and Pandas) [75 points total]\n",
        "\n",
        "Now that you are familiar (or still unfamiliar) with the dataset, we will now introduce you to SQL, or more specifically **pandasql**: a package created to allow users to query pandas DataFrames with SQL statements.\n",
        "\n",
        "The typical flow to use pandasql (shortened to **ps**) is as follows:\n",
        "1. Write a SQL query in the form of a string (Tip: use triple quotes \"\"\"x\"\"\" to write multi-line strings)\n",
        "2. Run the query using **ps.sqldf(your_query, locals())**\n",
        "\n",
        "Pandasql is convenient in that it allows you to reference the dataframes that are currently defined in your notebook, so you will be able to fully utilize the dataframes `movies_df`, `reviews_df` and `streaming_df` that you have created above!\n",
        "\n",
        "Given that it is a brand new language, we wanted to give you a chance to directly compare the similarities/differences of the pandas that you already know and the SQL that you are about to learn. Thus, for each of the simpler queries, we ask that you **look into the question twice: once with pandas and once with pandasql**. \n",
        "\n",
        "Each answer will thus require both a `pd_` and `sql_` prefixed-dataframe that you will submit seperately to the autograder. **We will be reviewing your code to make sure you wrote the code in the corresponding languages.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1 Movie Recommendations"
      ],
      "metadata": {
        "id": "8gIjaNk9BnJ-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geYyH57csade"
      },
      "source": [
        "#### 2.1.1 What movies have good reviews from critics and audience?\n",
        "\n",
        "`movies_df` contains all sorts of movies. We all love good movies, so let's try to separate the good from the bad.\n",
        "\n",
        "Rotten Tomatoes offers two major aggregate ratings: \n",
        "- one by critics, in the column `tomatometer_status`, and has three possible values: `Certified-Fresh`,`Fresh`, and `Rotten`.\n",
        "- the other by audience members, in the column `audience_status`, and has two possible values: `Upright` (good) and `Spilled` (bad).\n",
        "\n",
        "**TODO:** Using **pandas**, filter out movies from `movies_df` that are `Certified-Fresh` or `Fresh` into a new dataframe named `good_critics_df`, which should have the following schema:\n",
        "\n",
        ">rotten_tomatoes_link | movie_title\n",
        ">--- | ---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aZCy0IZR7v2"
      },
      "source": [
        "# TODO: Use pandas to obtain good_critics_df\n",
        "good_critics_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmRuF4phW_CT"
      },
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_good_critics_df', answer = good_critics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then, let's consider the ratings by movie-goers.\n",
        "\n",
        "**TODO:** Using **pandasql**, filter out movies from `movies_df` that are rated as `Upright` by the audience, into a new dataframe named `good_audience_df`, which should have the following schema:\n",
        "\n",
        ">rotten_tomatoes_link | movie_title\n",
        ">--- | ---"
      ],
      "metadata": {
        "id": "Hj58V6drh88p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsjMINr3X1zk"
      },
      "source": [
        "# TODO: Use pandasql to obtain good_audience_df\n",
        "good_audience_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "good_audience_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKddWLnIIF7v"
      },
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_good_audience_df', answer = good_audience_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHzBYzlDYoLx"
      },
      "source": [
        "Now, we can simply join these dataframes to get a table of all the best movies and binge watch one every night! But wait! Sometimes, the opinions of critics and audience may differ, so we need to make sure that the good movies we watch are ones that are viewed positively by both critics and audiences. \n",
        "\n",
        "**TODO**: Using **pandas and pandasql**, filter out movies with **mixed reviews** (ie. audience and critic opinions differ) to a new dataframe `pd/sql_mixed_movies_df`, ordered by lexicographic order of `movie_title`.\n",
        "\n",
        "For this question, you should **NOT** use `movies_df` or `exploded_movies_df`.\n",
        "\n",
        "Some tips:\n",
        "* For pandas, explore the `indicator` parameter within the `merge()` function.\n",
        "* For pandasql, considering using `EXCEPT`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2FvwI2CaA0X"
      },
      "source": [
        "# TODO: Filter out movies with mixed reviews using pandas\n",
        "pd_mixed_movies_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 points\n",
        "grader.grade(test_case_id = 'test_pd_mixed_movies', answer = pd_mixed_movies_df)"
      ],
      "metadata": {
        "id": "_ZAJ1otoIktJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08AFCWPEYlKW"
      },
      "source": [
        "# TODO: Filter out movies with mixed reviews from good_critics_df and good_audience_df\n",
        "mixed_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "sql_mixed_movies_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 points\n",
        "grader.grade(test_case_id = 'test_sql_mixed_movies', answer = (mixed_query,sql_mixed_movies_df))"
      ],
      "metadata": {
        "id": "7CDAyh7F1XiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:**\n",
        "\n",
        "Using **pandas/pandasql** and `pd/sql_mixed_movies_df`, find the top 10 mixed movies with the **largest absolute difference** in critic review score and audience review score. You should calculate this as the absolute difference between `tomatometer_rating` and `audience_rating`, and store this in a column named `diff`. If multiple movies have the same `diff` value, order their rows in lexicographic order.\n",
        "\n",
        "Format the output as a dataframe called `pd/sql_top_10_mixed_movies_df` that has the following schema:\n",
        "\n",
        ">movie_title | diff\n",
        ">--- | ---\n"
      ],
      "metadata": {
        "id": "8xcASc2QFZaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: pandas\n",
        "pd_top_10_mixed_movies_df = ..."
      ],
      "metadata": {
        "id": "bH8hYj3iHGxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: pandasql\n",
        "top_10_mixed_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "sql_top_10_mixed_movies_df = ..."
      ],
      "metadata": {
        "id": "5fuKqn8YLnOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 points\n",
        "grader.grade(test_case_id = 'test_top_10_mixed', answer = (top_10_mixed_query,pd_top_10_mixed_movies_df,sql_top_10_mixed_movies_df))"
      ],
      "metadata": {
        "id": "IR5WxqBK70Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooVqjTGm9eCA"
      },
      "source": [
        "#### 2.1.2 What movies can we recommend to Professor Davidson?\n",
        "\n",
        "Professor Davidson wants to take a break from teaching CIS 545. She is considering watching a movie on her Netflix account, and is gathering movie recommendations from you.\n",
        "\n",
        "Because her favourite holiday is halloween, Professor Davidson is looking to watch a horror movie. But first, she wants to answer a long-standing question in her mind: are there more horror movies during October? \n",
        "\n",
        "**TODO**: Using **pandas and pandasql**,\n",
        "- add a column `month` to movies_df that contains the month of the movie's original date of release.\n",
        "- Find the number of movies classified as `Horror`, grouped by month. Store this number in a column `num_movies` in descending order, and store the results in `pd/sql_horror_df` which should have the following format:\n",
        "\n",
        ">month | num_movies\n",
        ">--- | ---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djfer4T0uFa4"
      },
      "source": [
        "# TODO: pandas version\n",
        "pd_horror_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRewUlHAvGnR"
      },
      "source": [
        "# TODO: pandasql version\n",
        "horror_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "sql_horror_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdqFmCL6xBPv"
      },
      "source": [
        "# 4 points\n",
        "grader.grade(test_case_id = 'test_horror_month', answer = (horror_query, pd_horror_df, sql_horror_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW4VQc6BgD-6"
      },
      "source": [
        "It seems like October is not the most popular month in which horror movies are released. \n",
        "\n",
        "**TODO:**\n",
        "Using **pandas and pandasql**, find the movies classified as `Horror` that were originally released in **January**. Store the results in `pd/sql_jan_df` which should have the following format, sorted in descending order of `audience_count`:\n",
        "\n",
        ">rotten_tomatoes_link | movie_title | audience_count\n",
        ">--- | --- | ---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JULj_WXFPIuQ"
      },
      "source": [
        "# TODO: pandas version\n",
        "pd_jan_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EMJdmivPIuR"
      },
      "source": [
        "# TODO: pandasql version\n",
        "jan_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "sql_jan_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdUsAosuSq1Z"
      },
      "source": [
        "# 4 points\n",
        "grader.grade(test_case_id = 'test_horror_jan', answer = (jan_query, pd_jan_df, sql_jan_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqlpHrijkIEY"
      },
      "source": [
        "With all the information about these movies, we are ready to help Professor Davidson find her dream movie. In particular, she wants a movie that: \n",
        "- is a horror movie\n",
        "- was originally released in January \n",
        "- has been watched by at least 50000 audience members, with an overall `audience_rating` of `Upright`.  \n",
        "\n",
        "**TODO**:\n",
        "* Using the information provided above, and using **pandasql**, find the top 10 movies that fit her criteria, sorted by descending order of `audience_count`. Store this in a dataframe called `movie_recs_df`, that has the following schema:\n",
        "\n",
        ">rotten_tomatoes_link | movie_title | audience_count\n",
        ">--- | --- | ---\n",
        "\n",
        "**Hint**: you may find it helpful to leverage `good_audience_df` and `pd_jan_df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHqwEGTCkDU1"
      },
      "source": [
        "# TODO: use pandasql to find top 10 movies that fit criteria\n",
        "recs_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "movie_recs_df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1wy0ftelRit"
      },
      "source": [
        "# 6 points\n",
        "grader.grade(test_case_id = 'test_recs', answer = (recs_query, movie_recs_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Good Critic Reviews \n",
        "\n",
        "Now let us switch gears and examine `reviews_df`. In particular, we want to find the critics who are good at writing reviews.\n",
        "\n",
        "The first criteria that defines a good reviewer is balance and objectivity. Just like people (other than me), no movie is perfect, and so we probably want reviews to cover both the good and the bad. \n",
        "\n",
        "**TODO**: Using `reviews_df` and **pandasql**, find the reviews that contain both the word `\"good\"` and the word `\"bad\"` (case-insensitive). Include substrings (ie. it can be part of a word, such as '**good**-natured' or 'for**bad**e').\n",
        "\n",
        "Store the results in `balanced_df`, ordered by lexicographical order of `review_content`, with the following schema:\n",
        "\n",
        ">critic_name | review_content | \n",
        ">--- | --- | \n",
        "\n",
        "HINT: This involves checking for string similarity.\n",
        "- for Pandas, you can use str.contains()\n",
        "- for Pandasql, you may take a look at `LIKE()` and the wildcards `_` and/or `%`.\n",
        "\n"
      ],
      "metadata": {
        "id": "RAGGwxXzUvqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use pandasql to find reviews with both words \"good\" and \"bad\"\n",
        "balanced_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "balanced_df = ..."
      ],
      "metadata": {
        "id": "089mErwoUvqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_balanced_df', answer = (balanced_df, balanced_query))"
      ],
      "metadata": {
        "id": "X0m5C19CUvqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Good cause, bad movie.* One of my favourite quotes.\n",
        "\n",
        "The second criteria we have for good critics is tenure. Let us find the \"seasoned\" reviewers, ie. those who have been writing reviews for a long period of time.\n",
        "\n",
        "**TODO**: Using `reviews_df` and **pandasql**:\n",
        "- Create a column `date_diff` which contains the difference (in number of days) between each critic's first review and most recent review\n",
        "- Only include reviews from the 21st Century (ie. published from 2000 onward)\n",
        "- Only include critics whose first and most recent reviews are at least 10 years **(3652 days)** apart\n",
        "- Do not include reviews by `'Anonymous'`\n",
        "- Make sure your `date_diff` column contains integer values (HINT: see `CAST()`).\n",
        "\n",
        "You may consider casting the dates into a [format](https://www.sqlite.org/lang_datefunc.html) that will help produce the difference in days. \n",
        "\n",
        "Store the results in `critics_time_df`, ordered by descending order of `date_diff`, with the following schema:\n",
        "\n",
        ">critic_name | date_diff | \n",
        ">--- | --- | "
      ],
      "metadata": {
        "id": "-SfmLpQWOtS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use pandasql to find seasoned reviewers\n",
        "time_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "critics_time_df = ..."
      ],
      "metadata": {
        "id": "SiYCPV3KXO3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 points\n",
        "grader.grade(test_case_id = 'test_critic_review_time', answer = (critics_time_df, time_query))"
      ],
      "metadata": {
        "id": "iMQicmZgB8Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 What do we watch on Netflix tonight?\n",
        "\n",
        "Now let's run a tougher analysis. I'm sure we've all experienced the aimless scrolling through the Netflix homepage, trying (and sometimes failing) to find a movie that interests us. It's painful to manually search up the reviews of each movie we see, so why don't we try to automate this process? Let's determine the best on Netflix for each genre using both critics' and audiences' opinions on Rotten Tomatoes.\n",
        "\n",
        "This (and the next 2 questions) will require you to write a [nested SQL query](https://learnsql.com/blog/sql-nested-select/). That is, there will be at least one SELECT statement inside of another SELECT statement. This means that you should **NOT** write two separate SQL commands and call ps.sqldf() twice. \n",
        "\n",
        "**TODO**: Using `exploded_movies_df` and `streaming_df`:\n",
        "- Find all movies on Netflix, then **for each genre,** identify the ones with the highest combined RT tomatometer and audience rating (include all ties). Call this new column `combined_rt_rating`.\n",
        "- Rename the key `genre_category`, and sort your final result by this column in ascending order.\n",
        "\n",
        "Though it could be helpful, **you do NOT have to implement this in pandas**. Store the results in `netflix_best_df`, which should have the following format:\n",
        "\n",
        ">movie_title | genres | genre_category | combined_rt_rating\n",
        ">--- | --- | --- | ---\n",
        "\n",
        "where `genres` represents all the genres of the movie, and `genre_category` represents the genre that the movie's RT ratings are compared to."
      ],
      "metadata": {
        "id": "b_D-YM4gWLdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use pandasql to find the best on Netflix\n",
        "netflix_best_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "netflix_best_df = ..."
      ],
      "metadata": {
        "id": "nrsP2KQkOviH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 points\n",
        "grader.grade(test_case_id = 'test_netflix_best', answer = (netflix_best_query, netflix_best_df))"
      ],
      "metadata": {
        "id": "Vyj46kAZ2MQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Which months of the year does each streaming service have the most number of movies available?\n",
        "\n",
        "Maybe you don't care about picking out the \"good\" movies from the \"bad\" ones. Maybe you just want to watch as many movies as possible each month. Still, paying those monthly bills for all these streaming services (which seem to be increasing in number every day) doesn't seem to be a sustainable strategy. It would be much easier to pay for certain streaming services in the month they have the most movies.\n",
        "\n",
        "**TODO**: Using `movies_df` and `streaming_df`,\n",
        "- Count the number of movies for each streaming service per month using the `month` column in `movies_df` you created earlier.\n",
        "- For each streaming service, find the month in which they have the most movies available and how many movies are available in that month. Include ties, if there are any.\n",
        "\n",
        "Store the results in `best_streaming_df`, which should have the following format:\n",
        "\n",
        ">streaming_service | max_month | num_movies\n",
        ">--- | --- | --- |\n",
        "\n",
        "For example, if Netflix has the most movies (100) in October and Hulu has the most movies (90) in January, your query should output:\n",
        "\n",
        "streaming_service  | max_month        | num_movies\n",
        "-------------------|------------------|--------------\n",
        "Netflix    | 10                       | 100\n",
        "Hulu       | 1                        | 90\n",
        "\n",
        "and so on, for all four streaming services.\n",
        "\n",
        "**Hints:**\n",
        "- Make use of `UNION ALL` to stack tables with the same column names."
      ],
      "metadata": {
        "id": "6NJEomBiFNB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use pandasql to find the month where each streaming service has the most movies\n",
        "best_streaming_query = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "best_streaming_df = ..."
      ],
      "metadata": {
        "id": "xzFrXckUFMAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 points\n",
        "grader.grade(test_case_id = 'test_best_streaming', answer = (best_streaming_query, best_streaming_df))"
      ],
      "metadata": {
        "id": "B6INUivb2Khz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 What movies are underrated by critics (ft. top critics)?\n",
        "\n",
        "[According to the Rotten Tomatoes website](https://www.rottentomatoes.com/critics/top_critics), \"Top Critic is a designation created to distinguish Tomatometer-approved critics who excel at their craft. Critics selected are well-established, influential, and prolific...\" Although all critics on RT are officially approved, we want to establish a top-critic-only score that only includes the opinions of the \"cream of the crop\". Certain films could be poorly received by regular critics, but top critics see something special in them. Let's try and find these underappreciated movies!\n",
        "\n",
        "**TODO**: Using `reviews_df` and `movies_df`:\n",
        "- Filter out all non-top critics, then compute a new Tomatometer rating for each movie based on its proportion of Fresh reviews to total reviews (*both statistics among top critics only*). Call this new column `top_critic_tomatometer`, and **round this percentage to the nearest integer.**\n",
        "- Constrain the movie pool to **only ones that have more than 5 top critic reviews** to mitigate outlier impact.\n",
        "- Retrieve the top 10 movies with the highest difference between `top_critic_tomatometer` and its provided Tomatometer rating (`tomatometer_rating`) - name this difference `score_diff`. \n",
        "  - `top_critic_tomatometer` - `tomatometer_rating`. Not the absolute value of the difference.\n",
        "\n",
        "Store the results in `underrated_movies_df`, which should have the following format:\n",
        "\n",
        ">movie_title | top_critic_tomatometer | tomatometer_rating | score_diff\n",
        ">--- | --- | --- | ---\n",
        "\n",
        "**Hints:**\n",
        "- SQLite stores booleans as integers 1 (True) and 0 (False), respectively.\n",
        "- Tomatometer ratings are provided as percentages (i.e. 84 not 0.84), so make sure your top critic scores are as well.\n",
        "- Since there are numerous remakes included in the dataset with the same name as the original/multiple other versions, make sure you conduct aggregate/join operations on `rotten_tomatoes_link` to ensure uniqueness.\n",
        "- Dividing two integers in SQL yields a rounded integer."
      ],
      "metadata": {
        "id": "t3mlQcJhtlgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use pandasql to find the regular critics' underrated movies\n",
        "underrated_movies_query = \"\"\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x2V5Lnk9toNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** This query should take no more than a minute to run. If yours has been running for longer than a few minutes, then you're probably on the wrong track."
      ],
      "metadata": {
        "id": "ui1DHXOxLN7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "underrated_movies_df = ..."
      ],
      "metadata": {
        "id": "6zkhy0-qVQBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3P9WxuSU_Yc"
      },
      "source": [
        "# 15 points\n",
        "grader.grade(test_case_id = 'test_underrated_movies', answer = (underrated_movies_query, underrated_movies_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbNUhlz9ftHI"
      },
      "source": [
        "## Part 3: Working with Text Data [16 points]\n",
        "\n",
        "Shifting gears, let's now try to do some text-based analysis. Text data is complex, but can also be used to generate extremely interpretable results, making it valuable and interesting. \n",
        "\n",
        "Throughout this section, we will attempt to answer the following question:\n",
        "\n",
        "**How do reviews by top critics differ from reviews made by non-top critics?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9bd5kwIENXe"
      },
      "source": [
        "###3.1 Extract Data\n",
        "**TODO**: Using `reviews_df`, create two dataframes:\n",
        "* `top_critics_df`: where `top_critic` is True\n",
        "* `regular_critics_df`: where `top_critic` is False.\n",
        "\n",
        "Then, for each dataframe, convert the `review_content` column into two lists called `top_content` and `regular_content`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flAxhXQUpFDo"
      },
      "source": [
        "# TODO: create two dataframes & two lists\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQhF6QxnFAx7"
      },
      "source": [
        "# 1 point\n",
        "grader.grade(test_case_id = 'test_top_content_df', answer = len(top_content))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj9v2Gq2FNfm"
      },
      "source": [
        "# 1 point\n",
        "grader.grade(test_case_id = 'test_regular_content_df', answer = len(regular_content))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwLCBeTOLy3"
      },
      "source": [
        "###3.2 Tokenize the Text\n",
        "\n",
        "Here, we are going to split up the content into a list of words. We will use the **nltk** package, which contains an extensive set of tools to process text. Of course, like regex, this homework would be miles long if we really went into detail, so we are only going to utilize the following components:\n",
        "- `nltk.word_tokenize()`: a function used to tokenize our text\n",
        "- `nltk.corpus.stopwords`: a list of commonly used words such as \"a\",\"an\",\"in\" that are often ignored in text-related analysis\n",
        "\n",
        "Note that prior to this step for text analysis, we would typically clean the text first using regex. We didn't have to do that because the dataset was already well-cleaned, bt keep that in mind.\n",
        "\n",
        "**TODO:** First, use **stopwords** to create a set of the most common english stopwords. Then, implement **tokenized_content(content)** that takes in a content string and:\n",
        "1. tokenizes the text\n",
        "2. lowercases the token\n",
        "3. removes stop words (commonly used words such as \"a\",\"an\", \"in\")\n",
        "4. keeps words with only alphabet characters (no punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnFn2xJY5YK_"
      },
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUDt0d68oCfu"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "# Note that stopwords are all in lowercase format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bLwPIx2nPy-"
      },
      "source": [
        "# TODO: Create tokenized_content(content) function\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es5zPleHSROw"
      },
      "source": [
        "Now, apply your `tokenize_content()` function to each piece of content in **top_content** and **regular_content** and flatten both of the lists to create **top_tokens** and **regular_tokens**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F5mW4qoq2n8"
      },
      "source": [
        "# TODO: tokenize and flatten\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7PcX0mtG40R"
      },
      "source": [
        "# 1 point\n",
        "grader.grade(test_case_id = 'test_top_tokens', answer = len(top_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVQN15_bG5Af"
      },
      "source": [
        "# 1 point\n",
        "grader.grade(test_case_id = 'test_regular_tokens', answer = len(regular_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT0sTQHvp5E6"
      },
      "source": [
        "### 3.3 Most Frequent Words\n",
        "**TODO**: Now, find the 20 most common words amongst the content of `top_tokens` and `regular_tokens`. Return this as a list of `(word, count)` tuples, in descending order of `count`.\n",
        "\n",
        "**Hint**: You can use `Counter` in this question: https://docs.python.org/2/library/collections.html#counter-objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB-DiBperGEU"
      },
      "source": [
        "# TODO: Find 20 most common words amongst the content of the top and regular critic reviews\n",
        "top_most_common = ...\n",
        "regular_most_common = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VPDgzbuIyKO"
      },
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_top_most_common', answer = top_most_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmPqBlD_I0Og"
      },
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_regular_most_common', answer = regular_most_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvwIdmhNZmhY"
      },
      "source": [
        "###3.4 Refining our Lists\n",
        "\n",
        "Hmmm...both of these lists seem to display similar words. Let's try to tease out words that distinguish the high from the low scoring questions. \n",
        "\n",
        "One approach would be to find words in one list that are not in the other. This, however, may be too naive, as even if a word is extremely common in our high list, if it appears only once in our low list, it would get removed from consideration.\n",
        "\n",
        "Let's instead find the difference between the counts within our two lists. Thus, if a word is really common in one, but not the other, the count would only decrease slightly. Alternatively, if a word is common in both lists, it would effectively zero out. \n",
        "\n",
        "However, given that the number of regular tokens is about three times that of top tokens, we need to make sure both lists are of equal length before applying the difference method. \n",
        "\n",
        "**TODO:** Randomly sample 1 million (1000000) tokens from `top_tokens` and `regular_tokens`, and place them in lists `top_sample` and `regular_sample` respectively.\n",
        "-Then, using the difference method, create **distinct_top_counter** and **distinct_regular_counter** to find the top 20 counts of words within each group. Pass this into two lists, `distinct_top_most_common` and `distinct_regular_most_common` respectively. These should be lists of `(word, count)` tuples (`.most_common()` should help!)\n",
        "\n",
        "Be careful on which list you are subtracting!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyjdbIgmrQ--"
      },
      "source": [
        "# TODO: randomly sample 1 million tokens from top_tokens and regular_tokens\n",
        "top_sample = ...\n",
        "regular_sample = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDn4bzPLd2L"
      },
      "source": [
        "# TODO: use difference to find the top 20 counts of words within each group\n",
        "distinct_top_most_common = ...\n",
        "distinct_regular_most_common = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "279IYd6jL12_"
      },
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_distinct_top_most_common', answer = distinct_top_most_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byiqTRGuL2E3"
      },
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_distinct_regular_most_common', answer = distinct_regular_most_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEAi4a6OcjE4"
      },
      "source": [
        "### 3.5 Word Clouds\n",
        "\n",
        "Before we move on from this dataset, let's do one final step and visualize our results with wordclouds.\n",
        "\n",
        "**TODO**: Take a look at [this documentation](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html) and create two word clouds for our two groups of distinct tokens.\n",
        "\n",
        "Be sure to create these on the list of distinct tokens that you randomly sampled, and not just the top 20. We will be going through your notebooks and manually grading your world clouds (worth 4 points combined). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yt7PRqUrb8Z"
      },
      "source": [
        "# TODO: make a word cloud for top tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMfxDjbRreF1"
      },
      "source": [
        "# TODO: make a word cloud for regular tokens\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW Submission\n",
        "\n",
        "<br>\n",
        "<center><img src = \"https://memegenerator.net/img/instances/73124265/good-job.jpg\" width= \"500\" align =\"center\"/></center>\n",
        "<br>\n",
        "\n",
        "Big congratulations for getting this far! The piece of good news is that similar to HW1, you basically know the score you have when you submit to Gradescope. However, this time, we will be manually grading your wordclouds, so the autograder score is not final! Remember also that since we'll be checking for plagiarism, make sure to cite your sources (if any) using simple urls / links.\n",
        "\n",
        "Before you submit on Gradescope (you must submit your notebook to receive credit):\n",
        "\n",
        "1.   Restart and Run-All to make sure there's nothing wrong with your notebook\n",
        "2.   **Double check that you have the correct PennID (all numbers) in the autograder**. \n",
        "3. Make sure you've run all the PennGrader cells (and gotten the score that you want and deserve)\n",
        "4. Go to the \"File\" tab at the top left, and click \"Download .ipynb\" + \"Download .py\" and upload both the Python file and ipnyb notebook to Gradescope directly, naming the files **\"homework2.ipynb\"** and **\"homework2.py\"** respectively!\n",
        "\n",
        "###Be sure to name your files correctly!!!\n",
        "\n",
        "**Let the course staff know ASAP if you have any issues submitting.**"
      ],
      "metadata": {
        "id": "1L2nBxkcOnma"
      }
    }
  ]
}